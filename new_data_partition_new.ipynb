{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P300 Speller Dataset Processing\n",
    "\n",
    "This notebook processes the P300 Speller dataset collected from ALS patients. The dataset contains EEG recordings of P300 evoked potentials using the BCI2000 system with a 6x6 matrix of characters.\n",
    "\n",
    "## Dataset Overview\n",
    "- 8 ALS patients focused on characters in a 6x6 matrix\n",
    "- EEG recorded from 8 channels (Fz, Cz, Pz, Oz, P3, P4, PO7, PO8)\n",
    "- Sampling rate: 256 Hz\n",
    "- Data labeled as target (P300 present) and non-target (P300 absent) stimuli\n",
    "- Each character selection involves multiple row/column intensifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for data processing\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants Based on the P300 Dataset Documentation\n",
    "\n",
    "These constants are defined based on the P300 dataset documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset constants\n",
    "SAMPLE_RATE = 256  # Hz (sampling rate)\n",
    "SAMPLE_DURATION = 64  # Number of samples in each stimulus window\n",
    "INTENSIFIED_N_TIMES = 20  # Each item was intensified 20 times (10 row + 10 column intensifications)\n",
    "MATRIX_DIMENSIONS = 6  # 6x6 matrix of characters\n",
    "N_CHARACTERS = 36  # Total number of characters in the matrix\n",
    "N_CHANNELS = 8  # Number of EEG channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_matlab_file(file_path):\n",
    "    \"\"\"Load a MATLAB .mat file and return its contents.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the MATLAB file\n",
    "        \n",
    "    Returns:\n",
    "        dict: Contents of the MATLAB file\n",
    "    \"\"\"\n",
    "    return sio.loadmat(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(filename, data):\n",
    "    \"\"\"Save numpy array to a file.\n",
    "    \n",
    "    Args:\n",
    "        filename (str): Path to save the file\n",
    "        data (numpy.ndarray): Data to save\n",
    "    \"\"\"\n",
    "    np.save(filename, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(data, subject_id, samples_per_target, sample_offset=0):\n",
    "    \"\"\"Transform raw MATLAB data into structured arrays for target and non-target stimuli.\n",
    "    \n",
    "    Args:\n",
    "        data (dict): MATLAB data dictionary\n",
    "        subject_id (str): Subject identifier\n",
    "        samples_per_target (int): Number of samples to include per target\n",
    "        sample_offset (int, optional): Offset for sample selection. Defaults to 0.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Two arrays containing class 1 (non-target) and class 2 (target) data\n",
    "    \"\"\"\n",
    "    # Extract relevant data from the MATLAB structure\n",
    "    eeg_data = data['X'][0]  # EEG data [samples × channels]\n",
    "    stimulus_type = data['y'][0]  # Stimulus type (1=non-target, 2=target)\n",
    "    trial_start_indices = data['trial'][0][0]  # Trial start indices\n",
    "    \n",
    "    # Calculate total samples per trial\n",
    "    samples_per_trial = SAMPLE_DURATION * INTENSIFIED_N_TIMES * MATRIX_DIMENSIONS\n",
    "    \n",
    "    # Initialize arrays for non-target (class 1) and target (class 2) data\n",
    "    final_data_class1 = np.zeros((samples_per_target, samples_per_trial, N_CHANNELS))\n",
    "    final_data_class2 = np.zeros((samples_per_target, samples_per_trial, N_CHANNELS))\n",
    "    \n",
    "    # Counters for the number of samples in each class\n",
    "    class_1_count = 0\n",
    "    class_2_count = 0\n",
    "    \n",
    "    # Process each trial\n",
    "    for i, start_idx in enumerate(trial_start_indices):\n",
    "        # Define the trial window with padding\n",
    "        end_idx = start_idx + samples_per_trial + SAMPLE_RATE  # Add 1 second padding\n",
    "        trial_data = eeg_data[(start_idx + sample_offset):(end_idx + sample_offset)]\n",
    "        trial_stimulus_type = stimulus_type[(start_idx + sample_offset):(end_idx + sample_offset)]\n",
    "        \n",
    "        # Process each stimulus intensification in the trial\n",
    "        for j in range(INTENSIFIED_N_TIMES * MATRIX_DIMENSIONS):\n",
    "            # Get the stimulus type for this segment\n",
    "            current_stimulus_type = trial_stimulus_type[j * SAMPLE_DURATION : (j + 1) * SAMPLE_DURATION - 1]\n",
    "            \n",
    "            # Get the EEG data for this segment (including 1 second after stimulus)\n",
    "            # This captures the P300 response which typically occurs 300ms post-stimulus\n",
    "            character_data = trial_data[(j * SAMPLE_DURATION) : ((j * SAMPLE_DURATION) + SAMPLE_RATE), :]\n",
    "            \n",
    "            # Classify based on stimulus type\n",
    "            if 1 in current_stimulus_type:  # Non-target stimulus\n",
    "                if class_1_count < final_data_class1.shape[1]:\n",
    "                    final_data_class1[:, class_1_count] = character_data\n",
    "                    class_1_count += 1\n",
    "            elif 2 in current_stimulus_type:  # Target stimulus (P300 present)\n",
    "                if class_2_count < final_data_class2.shape[1]:\n",
    "                    final_data_class2[:, class_2_count] = character_data\n",
    "                    class_2_count += 1\n",
    "            else:\n",
    "                # Skip segments with no stimulus type information\n",
    "                continue\n",
    "    \n",
    "    # Trim arrays to actual sample counts\n",
    "    final_data_class1 = final_data_class1[:, :class_1_count]\n",
    "    final_data_class2 = final_data_class2[:, :class_2_count]\n",
    "    \n",
    "    print(f\"Class 1 (Non-target): {class_1_count} samples, Class 2 (Target): {class_2_count} samples\")\n",
    "    return final_data_class1, final_data_class2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Balancing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_classes(class1_data, class2_data):\n",
    "    \"\"\"Balance the two classes by downsampling the majority class.\n",
    "    \n",
    "    Args:\n",
    "        class1_data (numpy.ndarray): Data for class 1 (non-target)\n",
    "        class2_data (numpy.ndarray): Data for class 2 (target)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Balanced data for both classes\n",
    "    \"\"\"\n",
    "    # Get the shapes of both classes\n",
    "    class1_shape = class1_data.shape\n",
    "    class2_shape = class2_data.shape\n",
    "    \n",
    "    # Determine which class has fewer samples\n",
    "    if class1_shape[1] > class2_shape[1]:  # If class 1 has more samples\n",
    "        # Randomly downsample class 1 to match class 2 size\n",
    "        indices = np.random.choice(class1_shape[1], class2_shape[1], replace=False)\n",
    "        balanced_class1 = class1_data[:, indices, :]\n",
    "        balanced_class2 = class2_data\n",
    "        \n",
    "        print(f\"Downsampled class 1 from {class1_shape[1]} to {class2_shape[1]} samples\")\n",
    "    elif class2_shape[1] > class1_shape[1]:  # If class 2 has more samples\n",
    "        # Randomly downsample class 2 to match class 1 size\n",
    "        indices = np.random.choice(class2_shape[1], class1_shape[1], replace=False)\n",
    "        balanced_class1 = class1_data\n",
    "        balanced_class2 = class2_data[:, indices, :]\n",
    "        \n",
    "        print(f\"Downsampled class 2 from {class2_shape[1]} to {class1_shape[1]} samples\")\n",
    "    else:  # Classes already balanced\n",
    "        balanced_class1 = class1_data\n",
    "        balanced_class2 = class2_data\n",
    "        print(\"Classes already balanced with\", class1_shape[1], \"samples each\")\n",
    "    \n",
    "    return balanced_class1, balanced_class2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process and Save Data\n",
    "\n",
    "Load each subject's data, transform it, balance the classes, and save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of subject files\n",
    "subjects = [\"A01.mat\", \"A02.mat\", \"A03.mat\", \"A04.mat\", \"A05.mat\", \"A06.mat\", \"A07.mat\", \"A08.mat\"]\n",
    "\n",
    "# Create directories for saving processed data\n",
    "os.makedirs('./data/partitioned/class_1', exist_ok=True)\n",
    "os.makedirs('./data/partitioned/class_2', exist_ok=True)\n",
    "os.makedirs('./data/balanced/class_1', exist_ok=True)\n",
    "os.makedirs('./data/balanced/class_2', exist_ok=True)\n",
    "\n",
    "# Process each subject's data\n",
    "for file in subjects:\n",
    "    print(f\"\\nProcessing subject: {file}\")\n",
    "    # Load the MATLAB data\n",
    "    matlab_data = load_matlab_file(f\"data/raw/{file}\")['data'][0]\n",
    "    \n",
    "    # Transform the data\n",
    "    print(\"Transforming data...\")\n",
    "    class1_data, class2_data = transform_data(matlab_data, file.replace('.mat', ''), 256)\n",
    "    \n",
    "    # Verify class imbalance (there should be more non-target than target samples)\n",
    "    print(f\"Original data shapes - Class 1: {class1_data.shape}, Class 2: {class2_data.shape}\")\n",
    "    assert class1_data.shape[1] > class2_data.shape[1], \"Expected more non-target than target samples\"\n",
    "    \n",
    "    # Save the original (imbalanced) data\n",
    "    subject_id = file.replace(\".mat\", \"\")\n",
    "    save_data(f'./data/partitioned/class_1/{subject_id}', class1_data)\n",
    "    save_data(f'./data/partitioned/class_2/{subject_id}', class2_data)\n",
    "    print(f\"Saved original data for {subject_id}\")\n",
    "    \n",
    "    # Balance the classes\n",
    "    print(\"Balancing classes...\")\n",
    "    balanced_class1, balanced_class2 = balance_classes(class1_data, class2_data)\n",
    "    \n",
    "    # Verify balance\n",
    "    print(f\"Balanced data shapes - Class 1: {balanced_class1.shape}, Class 2: {balanced_class2.shape}\")\n",
    "    assert balanced_class1.shape[1] == balanced_class2.shape[1], \"Classes should have equal number of samples\"\n",
    "    \n",
    "    # Save the balanced data\n",
    "    save_data(f'./data/balanced/class_1/{subject_id}', balanced_class1)\n",
    "    save_data(f'./data/balanced/class_2/{subject_id}', balanced_class2)\n",
    "    print(f\"Saved balanced data for {subject_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_p300_average(subject_id):\n",
    "    \"\"\"Plot the average waveform for target and non-target stimuli for a subject.\n",
    "    \n",
    "    Args:\n",
    "        subject_id (str): Subject identifier (e.g., 'A01')\n",
    "    \"\"\"\n",
    "    # Load the balanced data\n",
    "    class1_data = np.load(f'./data/balanced/class_1/{subject_id}.npy')\n",
    "    class2_data = np.load(f'./data/balanced/class_2/{subject_id}.npy')\n",
    "    \n",
    "    # Average across trials for each class\n",
    "    avg_class1 = np.mean(class1_data, axis=1)  # Average non-target response\n",
    "    avg_class2 = np.mean(class2_data, axis=1)  # Average target response (P300)\n",
    "    \n",
    "    # Create a time vector (assuming 256 Hz sampling rate)\n",
    "    time = np.arange(avg_class1.shape[0]) / SAMPLE_RATE * 1000  # Convert to milliseconds\n",
    "    \n",
    "    # Plot averages for channel Pz (index 2), which typically shows the clearest P300\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(time, avg_class1[:, 2], 'b-', label='Non-Target')\n",
    "    plt.plot(time, avg_class2[:, 2], 'r-', label='Target (P300)')\n",
    "    plt.axvline(x=300, color='gray', linestyle='--', label='300ms (P300 expected)')\n",
    "    plt.xlabel('Time (ms)')\n",
    "    plt.ylabel('Amplitude (μV)')\n",
    "    plt.title(f'Average P300 Response for Subject {subject_id} (Channel Pz)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Plot P300 average for first subject\n",
    "plot_p300_average('A01')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}